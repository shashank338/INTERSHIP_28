{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea36da78",
   "metadata": {},
   "source": [
    "# 1.Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840517ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEADER TAGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From today's featured article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>On this day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today's featured picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Other areas of Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wikipedia's sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wikipedia languages</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     HEADER TAGS\n",
       "1           Welcome to Wikipedia\n",
       "2  From today's featured article\n",
       "3               Did you know ...\n",
       "4                    In the news\n",
       "5                    On this day\n",
       "6       Today's featured picture\n",
       "7       Other areas of Wikipedia\n",
       "8    Wikipedia's sister projects\n",
       "9            Wikipedia languages"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Here we checked respone found ok \n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "page \n",
    "\n",
    "# Here we get page content of all webpage\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "header_tags = []# empty list for store the L\n",
    "for i in soup.find_all(\"span\",class_=\"mw-headline\"):\n",
    "    header_tags.append(i.text)\n",
    "header_tags\n",
    "\n",
    "# Crearing the dataframe\n",
    "df=pd.DataFrame({'HEADER TAGS':header_tags})\n",
    "df.index +=1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a739ee",
   "metadata": {},
   "source": [
    "# 2.Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd70cb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLES</th>\n",
       "      <th>MOVIE RATINGS</th>\n",
       "      <th>YEAR OF RELESE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angry Men</td>\n",
       "      <td>8.9</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Jagten</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Everything Everywhere All at Once</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(2022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>M</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1931)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(1958)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                TITLES MOVIE RATINGS YEAR OF RELESE\n",
       "1             The Shawshank Redemption           9.2         (1994)\n",
       "2                        The Godfather           9.2         (1972)\n",
       "3                      The Dark Knight           9.0         (2008)\n",
       "4                        The Godfather           9.0         (1974)\n",
       "5                            Angry Men           8.9         (1957)\n",
       "..                                 ...           ...            ...\n",
       "96                              Jagten           8.3         (2012)\n",
       "97   Everything Everywhere All at Once           8.3         (2022)\n",
       "98                                   M           8.3         (1931)\n",
       "99                  North by Northwest           8.3         (1959)\n",
       "100                            Vertigo           8.2         (1958)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://www.imdb.com/chart/top/')\n",
    "page   #checked web page response range\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "movie_titles = []# empty list for store the \n",
    "for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    movie_titles.append(i.text.replace('\\n',''))\n",
    "movie_titles\n",
    "\n",
    "movie_ratings = []# empty list for store the \n",
    "for i in soup.find_all('strong'):\n",
    "    movie_ratings.append(i.text)\n",
    "movie_ratings\n",
    "\n",
    "year_relese = []# empty list for store the \n",
    "for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "    year_relese.append(i.text)\n",
    "year_relese\n",
    "\n",
    "df3=pd.DataFrame({'MOVIE RATINGS':movie_ratings,'YEAR OF RELESE':year_relese})# Creating the dataFrame \n",
    "df3\n",
    "df1=pd.DataFrame({'TITLES':movie_titles})#Here movie titles made dataframe due to remove unwanted characteristics from movie_titles\n",
    "df1\n",
    "df1=df1['TITLES'].str.extract(r'([A-Za-z]+(?: [A-Za-z]+)*)')#Here removed unwanted characteristics from movie_titles \n",
    "df1\n",
    "df2 = df1.set_axis(['TITLES'], axis=1, inplace=False)#Here changed the header after removed  characteristics from movie_titles \n",
    "df2\n",
    "df4=df2.join(df3)# after joins the dataframe2 to dataframe3 +\n",
    "df4              # the out put got \n",
    "df5=df4.iloc[:100]\n",
    "df5.index +=1 # Here the final DataFrame output shows\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbceb8d",
   "metadata": {},
   "source": [
    "# 3.Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1aa5910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLES</th>\n",
       "      <th>MOVIE RATINGS</th>\n",
       "      <th>YEAR OF RELESE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Masaan</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Kahaani</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Maheshinte Prathikaaram</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Baahubali</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2017)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TITLES MOVIE RATINGS YEAR OF RELESE\n",
       "1                   Jai Bhim           8.4         (2021)\n",
       "2                 Anbe Sivam           8.4         (2003)\n",
       "3                    Golmaal           8.4         (1979)\n",
       "4                    Nayakan           8.4         (1987)\n",
       "5          Pariyerum Perumal           8.4         (2018)\n",
       "..                       ...           ...            ...\n",
       "96                    Masaan           8.0         (2015)\n",
       "97            Dil Chahta Hai           8.0         (2001)\n",
       "98                   Kahaani           8.0         (2012)\n",
       "99   Maheshinte Prathikaaram           8.0         (2016)\n",
       "100                Baahubali           8.0         (2017)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as reqs\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "page = requests.get('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "page\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup     #checked web page response range\n",
    "\n",
    "movie_titles = []# empty list for store the \n",
    "for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    movie_titles.append(i.text.strip())\n",
    "    \n",
    "movie_titles\n",
    "\n",
    "movie_ratings=[]# empty list for store the \n",
    "for i in soup.find_all('strong'):\n",
    "    movie_ratings.append(i.text)\n",
    "\n",
    "movie_ratings\n",
    "\n",
    "year_relese = []# empty list for store the \n",
    "for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "    year_relese.append(i.text)\n",
    "\n",
    "year_relese\n",
    "\n",
    "df3=pd.DataFrame({'MOVIE RATINGS':movie_ratings,'YEAR OF RELESE':year_relese})\n",
    "df3\n",
    "\n",
    "df1=pd.DataFrame({'TITLES':movie_titles})#Here movie titles made dataframe due to remove unwanted characteristics from movie_titles\n",
    "df1\n",
    "df1=df1['TITLES'].str.extract(r'([A-Za-z]+(?: [A-Za-z]+)*)')#Here removed unwanted characteristics from movie_titles \n",
    "df1\n",
    "df2 = df1.set_axis(['TITLES'], axis=1, inplace=False)#Here changed the header after removed  characteristics from movie_titles \n",
    "df2\n",
    "df4=df2.join(df3)# after joins the dataframe2 to dataframe3 \n",
    "df4              # the out put got \n",
    "df5=df4.iloc[:100]\n",
    "df5.index+=1\n",
    "df5          # Here the final DataFrame output shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d868d",
   "metadata": {},
   "source": [
    "# 4.Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a76737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRESIDENTS NAME</th>\n",
       "      <th>TERM OF OFFICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>Term of Office: 25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>Term of Office: 25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>Term of Office: 25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>Term of Office: 25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>Term of Office: 25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>Term of Office: 25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>Term of Office: 25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>Term of Office: 25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>Term of Office: 24 August, 1974 to 11 February...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>Term of Office: 3 May, 1969 to 20 July, 1969 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>Term of Office: 13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>Term of Office: 13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>Term of Office: 26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                PRESIDENTS NAME  \\\n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13             Dr. Rajendra Prasad (1884-1963)    \n",
       "\n",
       "                                       TERM OF OFFICE  \n",
       "1     Term of Office: 25 July, 2012 to 25 July, 2017   \n",
       "2     Term of Office: 25 July, 2007 to 25 July, 2012   \n",
       "3     Term of Office: 25 July, 2002 to 25 July, 2007   \n",
       "4     Term of Office: 25 July, 1997 to 25 July, 2002   \n",
       "5     Term of Office: 25 July, 1992 to 25 July, 1997   \n",
       "6     Term of Office: 25 July, 1987 to 25 July, 1992   \n",
       "7     Term of Office: 25 July, 1982 to 25 July, 1987   \n",
       "8     Term of Office: 25 July, 1977 to 25 July, 1982   \n",
       "9   Term of Office: 24 August, 1974 to 11 February...  \n",
       "10  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
       "11        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
       "12       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
       "13   Term of Office: 26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "page    # Here we checked respone found ok\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "presidents_name = []# empty list for store the \n",
    "for i in soup.find_all('h3'):\n",
    "    presidents_name.append(i.text.replace('values',''))\n",
    "    \n",
    "presidents_name\n",
    "term_office = []# empty list for store the \n",
    "for i in soup.find_all('p'):\n",
    "    term_office.append(i.text)\n",
    "    \n",
    "term_office\n",
    "#creating the dataframe \n",
    "df=pd.DataFrame({'TERM OF OFFICE':term_office})\n",
    "df\n",
    "\n",
    "df1 = df.drop(df.index[[1, 3,5,16,17,18]])# Here we dropout the unwanted in term of office \n",
    "df1\n",
    "df1 = df1.reset_index(drop=True)# index reset done due to dataframe joining purpose \n",
    "df1\n",
    "#creating the dataframe\n",
    "df2=pd.DataFrame({'PRESIDENTS NAME':presidents_name})\n",
    "df2\n",
    "\n",
    "df3=df2.join(df1)#here joins the two dataframe\n",
    "df3.index +=1\n",
    "df3 # The final dataframe shows \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cc787",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed7e8",
   "metadata": {},
   "source": [
    "# a.Top 10 ODI teams in men’s cricket along with the records for matches, points and rating1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef485ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>MATCHS</th>\n",
       "      <th>POINTS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>12</td>\n",
       "      <td>1,505</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>19</td>\n",
       "      <td>2,353</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>1,929</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>22</td>\n",
       "      <td>2,304</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>16</td>\n",
       "      <td>1,635</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>19</td>\n",
       "      <td>1,872</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>24</td>\n",
       "      <td>2,275</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>24</td>\n",
       "      <td>2,086</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>26</td>\n",
       "      <td>1,885</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>15</td>\n",
       "      <td>986</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEAMS MATCHS POINTS RATINGS\n",
       "1    New Zealand     12  1,505     125\n",
       "2        England     19  2,353     124\n",
       "3      Australia     18  1,929     107\n",
       "4          India     22  2,304     105\n",
       "5       Pakistan     16  1,635     102\n",
       "6   South Africa     19  1,872      99\n",
       "7     Bangladesh     24  2,275      95\n",
       "8      Sri Lanka     24  2,086      87\n",
       "9    West Indies     26  1,885      73\n",
       "10   Afghanistan     15    986      66"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_TEAMS(url):\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    odi_team_page=requests.get(url)\n",
    "    odi_team_soup=BeautifulSoup(odi_team_page.content,'html.parser')\n",
    "    \n",
    "    odi_team=odi_team_soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text)\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    # here table showing diffrent and seperatly so extratcing required content\n",
    "    # both are seperalty extracted \n",
    "    \n",
    "    match_row1=odi_team_soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "    \n",
    "    for i in match_row1:\n",
    "        matches.append(i.text)\n",
    "\n",
    "               \n",
    "    # another row \n",
    "    match_row2=odi_team_soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    for i in range(0,len(match_row2),2):\n",
    "        matches.append(match_row2[i].text)\n",
    "    matches=matches[:10]\n",
    "    \n",
    "    \n",
    "    # the point also top and other rows is in diffrent \n",
    "    odi_point1=odi_team_soup.find_all('td',class_='rankings-block__banner--points')\n",
    "    \n",
    "    for i in odi_point1:\n",
    "        points.append(i.text)\n",
    "        \n",
    "    # here the another table points \n",
    "    odi_point2=odi_team_soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    for i in range(1,len(match_row2),2):\n",
    "        points.append(odi_point2[i].text)\n",
    "        points=points[:10]\n",
    "    \n",
    "    # the ratings also top and other rows is in diffrent \n",
    "    odi_rating=odi_team_soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    \n",
    "    # here the another table ratings\n",
    "    odi_rating=odi_team_soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "        ratings=ratings[:10]\n",
    "    \n",
    "    teams_odi=pd.DataFrame({})\n",
    "    teams_odi['TEAMS']=teams\n",
    "    teams_odi['MATCHS']=matches\n",
    "    teams_odi['POINTS']=points\n",
    "    teams_odi['RATINGS']=ratings\n",
    "    return(teams_odi)\n",
    "\n",
    "ODI_TEAM_MEN=ODI_TEAMS('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "ODI_TEAM_MEN.index+=1\n",
    "ODI_TEAM_MEN\n",
    " # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009608f",
   "metadata": {},
   "source": [
    "# b.Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b5c2415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYERS</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PLAYERS TEAMS RATINGS\n",
       "1              Babar Azam   PAK     891\n",
       "2             Virat Kohli   IND     811\n",
       "3             Imam-ul-Haq   PAK     795\n",
       "4            Rohit Sharma   IND     791\n",
       "5         Quinton de Kock    SA     789\n",
       "6          Jonny Bairstow   ENG     775\n",
       "7             Ross Taylor    NZ     775\n",
       "8   Rassie van der Dussen    SA     769\n",
       "9            David Warner   AUS     750\n",
       "10            Aaron Finch   AUS     745"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_BATSMEN(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    batsmen_page=requests.get(url)\n",
    "    batsmen_soup=BeautifulSoup(batsmen_page.content,'html.parser')\n",
    "    \n",
    "    # here taken first row from table \n",
    "    odi_player=batsmen_soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in odi_player:\n",
    "        players.append(i.text)\n",
    "    \n",
    "    # Another row table\n",
    "    odi_player=batsmen_soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in odi_player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "    \n",
    "    # here taken first row from table \n",
    "    odi_team=batsmen_soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "      # Another row table\n",
    "    odi_team=batsmen_soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    # First Row of the table# here taken first row from table \n",
    "    odi_rating=batsmen_soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    # Another row table\n",
    "    odi_rating=batsmen_soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    batsmen_odi=pd.DataFrame({})\n",
    "    batsmen_odi['PLAYERS']=players\n",
    "    batsmen_odi['TEAMS']=teams\n",
    "    batsmen_odi['RATINGS']=ratings\n",
    "    return(batsmen_odi)\n",
    "\n",
    "\n",
    "TOP_ODI_BATSMEN=ODI_BATSMEN('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "TOP_ODI_BATSMEN.index+=1\n",
    "TOP_ODI_BATSMEN   # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0f5c6",
   "metadata": {},
   "source": [
    "# c.Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b0083f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYERS</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLAYERS TEAMS RATINGS\n",
       "1        Trent Boult    NZ     726\n",
       "2       Chris Woakes   ENG     700\n",
       "3     Josh Hazlewood   AUS     698\n",
       "4         Matt Henry    NZ     683\n",
       "5   Mujeeb Ur Rahman   AFG     681\n",
       "6     Jasprit Bumrah   IND     679\n",
       "7     Shaheen Afridi   PAK     671\n",
       "8       Mehedi Hasan   BAN     661\n",
       "9    Shakib Al Hasan   BAN     657\n",
       "10       Rashid Khan   AFG     650"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_BOWLER(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    bowler_page=requests.get(url)\n",
    "    bowler_soup=BeautifulSoup(bowler_page.content,'html.parser')\n",
    "    \n",
    "    # here taken first row from table \n",
    "    odi_player=bowler_soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in odi_player:\n",
    "        players.append(i.text)\n",
    "    \n",
    "    # Another row table\n",
    "    odi_player=bowler_soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in odi_player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "    \n",
    "    # here taken first row from table \n",
    "    odi_team=bowler_soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    #  Another row table\n",
    "    odi_team=bowler_soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    # here taken first row from table \n",
    "    odi_rating=bowler_soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    #  Another row table\n",
    "    odi_rating=bowler_soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    bowler_odi=pd.DataFrame({})\n",
    "    bowler_odi['PLAYERS']=players\n",
    "    bowler_odi['TEAMS']=teams\n",
    "    bowler_odi['RATINGS']=ratings\n",
    "    return(bowler_odi)\n",
    "\n",
    "TOP_ODI_BOWLER=ODI_BOWLER('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "TOP_ODI_BOWLER.index+=1\n",
    "TOP_ODI_BOWLER  # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243596ec",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57b639",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7f6a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>MATCHS</th>\n",
       "      <th>POINTS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>29</td>\n",
       "      <td>3,551</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,531</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>29</td>\n",
       "      <td>2,889</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>28</td>\n",
       "      <td>1,882</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>6</td>\n",
       "      <td>249</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEAMS MATCHS POINTS RATINGS\n",
       "1      Australia     29  4,837     167\n",
       "2   South Africa     29  3,551     122\n",
       "3        England     30  3,531     118\n",
       "4          India     29  2,889     100\n",
       "5    New Zealand     31  3,019      97\n",
       "6    West Indies     30  2,768      92\n",
       "7     Bangladesh     12    930      78\n",
       "8       Pakistan     28  1,882      67\n",
       "9      Sri Lanka      6    249      42\n",
       "10      Zimbabwe      8      0       0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_WOMEN_TEAM(url):\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    odi_team_page=requests.get(url)\n",
    "    odi_team_soup=BeautifulSoup(odi_team_page.content,'html.parser')\n",
    "    \n",
    "    odi_team=odi_team_soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text)\n",
    "    teams=teams[:10]\n",
    "    \n",
    "     # here table showing diffrent and seperatly so extratcing required content\n",
    "     # both are seperalty extracted \n",
    "    match_row1=odi_team_soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "    for i in match_row1:\n",
    "        matches.append(i.text)\n",
    "\n",
    "               \n",
    "    # another rows table\n",
    "    match_row2=odi_team_soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    for i in range(0,len(match_row2),2):\n",
    "        matches.append(match_row2[i].text)\n",
    "    matches=matches[:10]\n",
    "\n",
    "    # here the ponts also in diffrent format and another table also \n",
    "    odi_point1=odi_team_soup.find_all('td',class_='rankings-block__banner--points')\n",
    "    for i in odi_point1:\n",
    "        points.append(i.text)\n",
    "        \n",
    "    odi_point2=odi_team_soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    for i in range(1,len(match_row2),2):\n",
    "        points.append(odi_point2[i].text)\n",
    "    points=points[:10]\n",
    "    \n",
    "    odi_rating=odi_team_soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    \n",
    "    odi_rating=odi_team_soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    \n",
    "    \n",
    "    teams_odi=pd.DataFrame({})\n",
    "    teams_odi['TEAMS']=teams\n",
    "    teams_odi['MATCHS']=matches\n",
    "    teams_odi['POINTS']=points\n",
    "    teams_odi['RATINGS']=ratings\n",
    "    return(teams_odi)\n",
    "ODI_TEAM_WOMEN=ODI_WOMEN_TEAM('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "ODI_TEAM_WOMEN.index+=1\n",
    "ODI_TEAM_WOMEN   # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2129cf2",
   "metadata": {},
   "source": [
    "# b.Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e42ecbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYERS</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mithali Raj</td>\n",
       "      <td>IND</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PLAYERS TEAMS RATINGS\n",
       "1        Alyssa Healy   AUS     785\n",
       "2      Natalie Sciver   ENG     750\n",
       "3         Beth Mooney   AUS     748\n",
       "4     Laura Wolvaardt    SA     722\n",
       "5         Meg Lanning   AUS     710\n",
       "6      Rachael Haynes   AUS     701\n",
       "7         Mithali Raj   IND     686\n",
       "8   Amy Satterthwaite    NZ     681\n",
       "9     Smriti Mandhana   IND     669\n",
       "10     Tammy Beaumont   ENG     659"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_WOMEN_PLAYER(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    batsmen_page=requests.get(url)\n",
    "    batsmen_soup=BeautifulSoup(batsmen_page.content,'html.parser')\n",
    "    \n",
    "    # here taken first row from table\n",
    "    odi_player=batsmen_soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in odi_player:\n",
    "        players.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Another row table\n",
    "    odi_player=batsmen_soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in odi_player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "    \n",
    "    # here taken first row from table\n",
    "    odi_team=batsmen_soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    # Another row table\n",
    "    odi_team=batsmen_soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    # here taken first row from table\n",
    "    odi_rating=batsmen_soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "   # Another row table\n",
    "    odi_rating=batsmen_soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    \n",
    "    \n",
    "    batsmen_odi=pd.DataFrame({})\n",
    "    batsmen_odi['PLAYERS']=players\n",
    "    batsmen_odi['TEAMS']=teams\n",
    "    batsmen_odi['RATINGS']=ratings\n",
    "    return(batsmen_odi)\n",
    "TOP_ODI_BATSMEN=ODI_WOMEN_PLAYER('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "TOP_ODI_BATSMEN.index+=1\n",
    "TOP_ODI_BATSMEN   # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07753406",
   "metadata": {},
   "source": [
    "# c.Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f01df4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYERS</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATINGS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLAYERS TEAMS RATINGS\n",
       "1     Natalie Sciver   ENG     393\n",
       "2       Ellyse Perry   AUS     374\n",
       "3     Marizanne Kapp    SA     359\n",
       "4    Hayley Matthews    WI     338\n",
       "5        Amelia Kerr    NZ     335\n",
       "6   Ashleigh Gardner   AUS     269\n",
       "7      Deepti Sharma   IND     249\n",
       "8      Jess Jonassen   AUS     245\n",
       "9    Katherine Brunt   ENG     221\n",
       "10    Jhulan Goswami   IND     217"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def ODI_WOMEN_ALLROUNDER_PLAYER(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    bowler_page=requests.get(url)\n",
    "    bowler_soup=BeautifulSoup(bowler_page.content,'html.parser')\n",
    "    \n",
    "   # here taken first row from table\n",
    "    odi_player=bowler_soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in odi_player:\n",
    "        players.append(i.text)\n",
    "    \n",
    "     # Another row table\n",
    "    odi_player=bowler_soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in odi_player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "    \n",
    "    # here taken first row from table\n",
    "    odi_team=bowler_soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "      # Another row table\n",
    "    odi_team=bowler_soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in odi_team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    # here taken first row from table\n",
    "    odi_rating=bowler_soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "     # Another row table\n",
    "    odi_rating=bowler_soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in odi_rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    \n",
    "    \n",
    "    bowler_odi=pd.DataFrame({})\n",
    "    bowler_odi['PLAYERS']=players\n",
    "    bowler_odi['TEAMS']=teams\n",
    "    bowler_odi['RATINGS']=ratings\n",
    "    return(bowler_odi)\n",
    "ODI_WOMEN_ALLROUNDER_PLAYER=ODI_WOMEN_ALLROUNDER_PLAYER('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "ODI_WOMEN_ALLROUNDER_PLAYER.index+=1\n",
    "ODI_WOMEN_ALLROUNDER_PLAYER  # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05551556",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1743f2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 How Russia could try to get around the European Union's oil sanctions | 55 Min Ago \n",
      " https://www.cnbc.com/2022/06/02/how-russia-could-try-to-get-around-the-european-unions-oil-sanctions.html \n",
      "\n",
      "2 First time on a yacht? Avoid these 7 amateur mistakes  | 2 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/first-time-on-a-yacht-avoid-these-7-amateur-mistakes-.html \n",
      "\n",
      "3 Oil slides after report Saudi Arabia could step up if Russian output dips under ban | 3 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/oil-prices-eu-sanctions-russia-saudi-arabia-output-opec.html \n",
      "\n",
      "4 Solana suffered its second outage in a month, sending price plunging | 4 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/solana-suffered-its-second-outage-in-a-month-sending-price-plunging.html \n",
      "\n",
      "5 Goldman names its top battery stock to play rising demand for electric vehicles  | 4 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/goldman-sachs-top-battery-stock-as-demand-for-electric-vehicles-rises.html \n",
      "\n",
      "6 'Fallen angels': Morgan Stanley says buy the dip on these 5 global stocks | 5 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/morgan-stanley-says-buy-the-dip-on-these-5-global-stocks.html \n",
      "\n",
      "7 Self-driving car companies' first step to making money isn't robotaxis | 5 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/self-driving-car-companies-first-step-to-making-money-isnt-robotaxis.html \n",
      "\n",
      "8 Javier Olivan, who's replacing Sandberg at Meta, built his career on global growth | 5 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/javier-olivan-metas-new-coo-built-his-career-on-global-growth.html \n",
      "\n",
      "9 Sandberg was Facebook's adult, but it's always been a Zuckerberg production | 6 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/sheryl-sandberg-was-adult-in-room-of-zuckerberg-production-at-facebook.html \n",
      "\n",
      "10 Hong Kong's Hang Seng index leads losses in Asia; oil prices drop close to 2% | 6 Hours Ago \n",
      " https://www.cnbc.com/2022/06/02/asia-markets-australia-april-trade-data-currencies-oil.html \n",
      "\n",
      "11 Cramer's lightning round: I like P&G over Olaplex | 7 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/cramers-lightning-round-i-like-pg-over-olaplex.html \n",
      "\n",
      "12 Biden administration will cancel student debt for half a million students   | 7 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/biden-administration-will-cancel-student-debt-for-half-a-million-students-from-corinthian-colleges-.html \n",
      "\n",
      "13 Jim Cramer likes this alternative energy play for a high inflation environment | 7 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/cramer-the-alternative-energy-atlantica-sustainable-infrastructure-high-inflation-environment.html \n",
      "\n",
      "14 Jim Cramer says investors should consider 3 things before buying a stock | 7 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/cramer-says-investors-should-consider-3-things-before-buying-a-stock.html \n",
      "\n",
      "15 Stock futures fall slightly as investors dwell on health of the economy | 8 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/stock-futures-are-little-changed-as-investors-dwell-on-health-of-the-economy.html \n",
      "\n",
      "16 Best trades on CNBC Wednesday: Safety stocks and Salesforce earnings reaction | 8 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/best-trades-on-cnbc-wednesday-safety-stocks-for-market-uncertainty.html \n",
      "\n",
      "17 Reckitt says it's ready to import 21 million bottles of baby formula with FDA OK | 8 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/reckitt-baby-formula-plants-can-produce-21-million-bottles-for-us.html \n",
      "\n",
      "18 Why Meta drop on Sandberg's exit is a good chance to snap up shares | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/why-drop-in-meta-on-sandbergs-exit-is-a-good-chance-to-snap-up-shares.html \n",
      "\n",
      "19 Stocks making the biggest moves after hours: Chewy, MongoDB and more | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/stocks-making-the-biggest-moves-after-hours-chewy-gamestop-and-more.html \n",
      "\n",
      "20 Tough new sanctions on Russia could change the OPEC+ dynamic  | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/tough-new-sanctions-on-russian-oil-could-change-the-opec-dynamic.html \n",
      "\n",
      "21 These 'dividend aristocrats' offer returns amid uncertainty, Credit Suisse says  | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/these-dividend-aristocrats-offer-steady-returns-in-an-uncertain-market-credit-suisse-says-.html \n",
      "\n",
      "22 GameStop reports $158 million loss and decline in hardware sales | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/gamestop-gme-earnings-q1-2022.html \n",
      "\n",
      "23 Despite high-profile staff cuts, job market remains ‘burning hot’ for workers | 9 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/job-market-remains-burning-hot-for-workers-in-latest-jolts-report.html \n",
      "\n",
      "24 Facebook parent Meta COO Sheryl Sandberg is stepping down | 10 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/facebook-coo-sheryl-sandberg-says-she-is-stepping-down.html \n",
      "\n",
      "25 These are JPMorgan's favorite stocks heading into June | 10 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/these-are-jpmorgans-favorite-stocks-heading-into-june.html \n",
      "\n",
      "26 Sweeping water restrictions begin in Southern California as drought worsens | 10 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/southern-california-water-restrictions-start-today-amid-drought.html \n",
      "\n",
      "27 Former OpenSea employee charged in first-ever NFT insider trading case | 11 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/former-opensea-employee-charged-in-first-ever-nft-insider-trading-case.html \n",
      "\n",
      "28 Here's what's in Biden's $700 million military aid package for Ukraine | 11 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/heres-whats-in-bidens-700-million-military-aid-package-for-ukraine.html \n",
      "\n",
      "29 Dialogue alone won't get more women into tech, says fintech CEO—here's what will | 11 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/fintech-ceo-dialogue-is-not-enough-to-get-more-women-into-tech.html \n",
      "\n",
      "30 Fed report sees 'slight or modest' economic growth as inflation surges | 11 Hours Ago \n",
      " https://www.cnbc.com/2022/06/01/fed-report-sees-slight-or-modest-economic-growth-as-inflation-surges.html \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NEWS TITLE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>NEWS LINK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Russia could try to get around the Europea...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/how-russia-cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First time on a yacht? Avoid these 7 amateur m...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/first-time-on-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oil slides after report Saudi Arabia could ste...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/oil-prices-eu-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solana suffered its second outage in a month, ...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/solana-suffere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Goldman names its top battery stock to play ri...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Fallen angels': Morgan Stanley says buy the d...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Self-driving car companies' first step to maki...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/self-driving-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Javier Olivan, who's replacing Sandberg at Met...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/javier-olivan-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sandberg was Facebook's adult, but it's always...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/sheryl-sandber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hong Kong's Hang Seng index leads losses in As...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/02/asia-markets-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cramer's lightning round: I like P&amp;G over Olaplex</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Biden administration will cancel student debt ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Jim Cramer likes this alternative energy play ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramer-the-alt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jim Cramer says investors should consider 3 th...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/cramer-says-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stock futures fall slightly as investors dwell...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/stock-futures-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Best trades on CNBC Wednesday: Safety stocks a...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/best-trades-on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Reckitt says it's ready to import 21 million b...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/reckitt-baby-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Why Meta drop on Sandberg's exit is a good cha...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/why-drop-in-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Stocks making the biggest moves after hours: C...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Tough new sanctions on Russia could change the...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/tough-new-sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>These 'dividend aristocrats' offer returns ami...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/these-dividend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GameStop reports $158 million loss and decline...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/gamestop-gme-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Despite high-profile staff cuts, job market re...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/job-market-rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Facebook parent Meta COO Sheryl Sandberg is st...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/facebook-coo-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>These are JPMorgan's favorite stocks heading i...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/these-are-jpmo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sweeping water restrictions begin in Southern ...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/southern-calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Former OpenSea employee charged in first-ever ...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/former-opensea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Here's what's in Biden's $700 million military...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/heres-whats-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Dialogue alone won't get more women into tech,...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/fintech-ceo-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Fed report sees 'slight or modest' economic gr...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/01/fed-report-see...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           NEWS TITLE          TIME  \\\n",
       "1   How Russia could try to get around the Europea...    55 Min Ago   \n",
       "2   First time on a yacht? Avoid these 7 amateur m...   2 Hours Ago   \n",
       "3   Oil slides after report Saudi Arabia could ste...   3 Hours Ago   \n",
       "4   Solana suffered its second outage in a month, ...   4 Hours Ago   \n",
       "5   Goldman names its top battery stock to play ri...   4 Hours Ago   \n",
       "6   'Fallen angels': Morgan Stanley says buy the d...   5 Hours Ago   \n",
       "7   Self-driving car companies' first step to maki...   5 Hours Ago   \n",
       "8   Javier Olivan, who's replacing Sandberg at Met...   5 Hours Ago   \n",
       "9   Sandberg was Facebook's adult, but it's always...   6 Hours Ago   \n",
       "10  Hong Kong's Hang Seng index leads losses in As...   6 Hours Ago   \n",
       "11  Cramer's lightning round: I like P&G over Olaplex   7 Hours Ago   \n",
       "12  Biden administration will cancel student debt ...   7 Hours Ago   \n",
       "13  Jim Cramer likes this alternative energy play ...   7 Hours Ago   \n",
       "14  Jim Cramer says investors should consider 3 th...   7 Hours Ago   \n",
       "15  Stock futures fall slightly as investors dwell...   8 Hours Ago   \n",
       "16  Best trades on CNBC Wednesday: Safety stocks a...   8 Hours Ago   \n",
       "17  Reckitt says it's ready to import 21 million b...   8 Hours Ago   \n",
       "18  Why Meta drop on Sandberg's exit is a good cha...   9 Hours Ago   \n",
       "19  Stocks making the biggest moves after hours: C...   9 Hours Ago   \n",
       "20  Tough new sanctions on Russia could change the...   9 Hours Ago   \n",
       "21  These 'dividend aristocrats' offer returns ami...   9 Hours Ago   \n",
       "22  GameStop reports $158 million loss and decline...   9 Hours Ago   \n",
       "23  Despite high-profile staff cuts, job market re...   9 Hours Ago   \n",
       "24  Facebook parent Meta COO Sheryl Sandberg is st...  10 Hours Ago   \n",
       "25  These are JPMorgan's favorite stocks heading i...  10 Hours Ago   \n",
       "26  Sweeping water restrictions begin in Southern ...  10 Hours Ago   \n",
       "27  Former OpenSea employee charged in first-ever ...  11 Hours Ago   \n",
       "28  Here's what's in Biden's $700 million military...  11 Hours Ago   \n",
       "29  Dialogue alone won't get more women into tech,...  11 Hours Ago   \n",
       "30  Fed report sees 'slight or modest' economic gr...  11 Hours Ago   \n",
       "\n",
       "                                            NEWS LINK  \n",
       "1   https://www.cnbc.com/2022/06/02/how-russia-cou...  \n",
       "2   https://www.cnbc.com/2022/06/02/first-time-on-...  \n",
       "3   https://www.cnbc.com/2022/06/02/oil-prices-eu-...  \n",
       "4   https://www.cnbc.com/2022/06/01/solana-suffere...  \n",
       "5   https://www.cnbc.com/2022/06/02/goldman-sachs-...  \n",
       "6   https://www.cnbc.com/2022/06/02/morgan-stanley...  \n",
       "7   https://www.cnbc.com/2022/06/02/self-driving-c...  \n",
       "8   https://www.cnbc.com/2022/06/01/javier-olivan-...  \n",
       "9   https://www.cnbc.com/2022/06/01/sheryl-sandber...  \n",
       "10  https://www.cnbc.com/2022/06/02/asia-markets-a...  \n",
       "11  https://www.cnbc.com/2022/06/01/cramers-lightn...  \n",
       "12  https://www.cnbc.com/2022/06/01/biden-administ...  \n",
       "13  https://www.cnbc.com/2022/06/01/cramer-the-alt...  \n",
       "14  https://www.cnbc.com/2022/06/01/cramer-says-in...  \n",
       "15  https://www.cnbc.com/2022/06/01/stock-futures-...  \n",
       "16  https://www.cnbc.com/2022/06/01/best-trades-on...  \n",
       "17  https://www.cnbc.com/2022/06/01/reckitt-baby-f...  \n",
       "18  https://www.cnbc.com/2022/06/01/why-drop-in-me...  \n",
       "19  https://www.cnbc.com/2022/06/01/stocks-making-...  \n",
       "20  https://www.cnbc.com/2022/06/01/tough-new-sanc...  \n",
       "21  https://www.cnbc.com/2022/06/01/these-dividend...  \n",
       "22  https://www.cnbc.com/2022/06/01/gamestop-gme-e...  \n",
       "23  https://www.cnbc.com/2022/06/01/job-market-rem...  \n",
       "24  https://www.cnbc.com/2022/06/01/facebook-coo-s...  \n",
       "25  https://www.cnbc.com/2022/06/01/these-are-jpmo...  \n",
       "26  https://www.cnbc.com/2022/06/01/southern-calif...  \n",
       "27  https://www.cnbc.com/2022/06/01/former-opensea...  \n",
       "28  https://www.cnbc.com/2022/06/01/heres-whats-in...  \n",
       "29  https://www.cnbc.com/2022/06/01/fintech-ceo-di...  \n",
       "30  https://www.cnbc.com/2022/06/01/fed-report-see...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup   # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "page \n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "# scrape data for latest news headline\n",
    "news = []\n",
    "for i in soup.find_all(\"a\",class_=\"LatestNews-headline\"):\n",
    "    news.append(i.text.replace('\\n',''))\n",
    "\n",
    "# scrape data for time of headline\n",
    "time = []\n",
    "for i in soup.find_all(\"time\",class_=\"LatestNews-timestamp\"):\n",
    "    time.append(i.text.replace('\\n',''))\n",
    "\n",
    "# scrape data for news link\n",
    "link = []\n",
    "for i in soup.find_all(\"a\",class_=\"LatestNews-headline\"):\n",
    "    link.append(i['href'])\n",
    "    \n",
    "# display news | time | news link\n",
    "for val in range(0,len(news)):\n",
    "    print(val+1,news[val],'|',time[val],'\\n',link[val],'\\n')\n",
    "db_news = pd.DataFrame({'NEWS TITLE':news,'TIME':time,'NEWS LINK':link})# Creating the dataframe \n",
    "db_news.index +=1\n",
    "db_news      # The final DataFrame shows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a30a1",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67ec3a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAPER TITLE</th>\n",
       "      <th>AUTHORS</th>\n",
       "      <th>PUBLISHED DATE</th>\n",
       "      <th>LINKS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          PAPER TITLE  \\\n",
       "1                                    Reward is enough   \n",
       "2                           Making sense of raw input   \n",
       "3   Law and logic: A review from an argumentation ...   \n",
       "4              Creativity and artificial intelligence   \n",
       "5   Artificial cognition for social human–robot in...   \n",
       "6   Explanation in artificial intelligence: Insigh...   \n",
       "7                       Making sense of sensory input   \n",
       "8   Conflict-based search for optimal multi-agent ...   \n",
       "9   Between MDPs and semi-MDPs: A framework for te...   \n",
       "10  The Hanabi challenge: A new frontier for AI re...   \n",
       "11  Evaluating XAI: A comparison of rule-based and...   \n",
       "12           Argumentation in artificial intelligence   \n",
       "13  Algorithms for computing strategies in two-pla...   \n",
       "14      Multiple object tracking: A literature review   \n",
       "15  Selection of relevant features and examples in...   \n",
       "16  A survey of inverse reinforcement learning: Ch...   \n",
       "17  Explaining individual predictions when feature...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  Integrating social power into the decision-mak...   \n",
       "20  “That's (not) the output I expected!” On the r...   \n",
       "21  Explaining black-box classifiers using post-ho...   \n",
       "22  Algorithm runtime prediction: Methods & evalua...   \n",
       "23              Wrappers for feature subset selection   \n",
       "24  Commonsense visual sensemaking for autonomous ...   \n",
       "25         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              AUTHORS  PUBLISHED DATE  \\\n",
       "1   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "2           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "3                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "4                                 Boden, Margaret A.      August 1998   \n",
       "5     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "6                                        Miller, Tim    February 2019   \n",
       "7   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "8   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "9   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "10        Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "11  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "12               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "13       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "14             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "15                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "16                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "17      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "18  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "19    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "20                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "21  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "22  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "23                      Kohavi, Ron, John, George H.    December 1997   \n",
       "24  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "25                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                                LINKS  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  \n",
       "25  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "page #checked web page response range\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "paper_title = []# empty list for store the \n",
    "for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
    "     paper_title.append(i.text)\n",
    "    \n",
    "paper_title\n",
    "\n",
    "authors = []# empty list for store the \n",
    "for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "     authors.append(i.text)\n",
    "    \n",
    "authors \n",
    "\n",
    "published_date = []# empty list for store the \n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "     published_date.append(i.text)\n",
    "    \n",
    "published_date\n",
    "\n",
    "links = []\n",
    "for link in soup.findAll('a'):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "links\n",
    "\n",
    "df=pd.DataFrame({'LINKS':links})# Here links dataframe done due to removing the unwanted links\n",
    "df\n",
    "v=df.iloc[48:] # unwanted links are removed from links\n",
    "v\n",
    "df2=v.iloc[:25]   # unwanted links are removed from links\n",
    "df2\n",
    "df2 = df2.reset_index(drop=True)#Here index of the link made reset starts from 0\n",
    "df2\n",
    "df1=pd.DataFrame({'PAPER TITLE':paper_title,'AUTHORS':authors,'PUBLISHED DATE':published_date })\n",
    "df1\n",
    "df3=df1.join(df2)# after joins the dataframe1 to dataframe2 the final dataframe\n",
    "df3.index +=1\n",
    "df3    # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af9245",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5a89bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retaurant</th>\n",
       "      <th>Cusine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Castle, Barbeque]</td>\n",
       "      <td>[₹ 2,000 for 2 (approx) ,  North Indian, Chinese]</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Jungle, Jamboree]</td>\n",
       "      <td>[₹ 1,400 for 2 (approx) ,  North Indian, Asian...</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Castle, Barbeque]</td>\n",
       "      <td>[₹ 2,000 for 2 (approx) ,  Chinese, North Indian]</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Cafe, Knosh]</td>\n",
       "      <td>[₹ 3,000 for 2 (approx) ,  Italian, Continental]</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[The, Barbeque, Company]</td>\n",
       "      <td>[₹ 1,700 for 2 (approx) ,  North Indian, Chinese]</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[India, Grill]</td>\n",
       "      <td>[₹ 2,400 for 2 (approx) ,  North Indian, Italian]</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Delhi, Barbeque]</td>\n",
       "      <td>[₹ 1,800 for 2 (approx) ,  North Indian]</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[The, Monarch, -, Bar, Be, Que, Village]</td>\n",
       "      <td>[₹ 1,900 for 2 (approx) ,  North Indian]</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[World, Cafe]</td>\n",
       "      <td>[₹ 2,400 for 2 (approx) ,  North Indian, Italian]</td>\n",
       "      <td>Vibe by The Lalit Traveller,Sector 35, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Indian, Grill, Room]</td>\n",
       "      <td>[₹ 2,200 for 2 (approx) ,  North Indian, Mughlai]</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Mad, 4, Bar, B, Que]</td>\n",
       "      <td>[₹ 800 for 2 (approx) ,  North Indian]</td>\n",
       "      <td>Sector 29, Faridabad</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Barbeque, 29]</td>\n",
       "      <td>[₹ 1,500 for 2 (approx) ,  North Indian, Mughl...</td>\n",
       "      <td>NIT, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Glasshouse]</td>\n",
       "      <td>[₹ 3,400 for 2 (approx) ,  European, Italian, ...</td>\n",
       "      <td>DoubleTree By Hilton Gurugram Baani Square,Sec...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Retaurant  \\\n",
       "S.No                                             \n",
       "1                           [Castle, Barbeque]   \n",
       "2                           [Jungle, Jamboree]   \n",
       "3                           [Castle, Barbeque]   \n",
       "4                                [Cafe, Knosh]   \n",
       "5                     [The, Barbeque, Company]   \n",
       "6                               [India, Grill]   \n",
       "7                            [Delhi, Barbeque]   \n",
       "8     [The, Monarch, -, Bar, Be, Que, Village]   \n",
       "9                                [World, Cafe]   \n",
       "10                       [Indian, Grill, Room]   \n",
       "11                       [Mad, 4, Bar, B, Que]   \n",
       "12                              [Barbeque, 29]   \n",
       "13                                [Glasshouse]   \n",
       "\n",
       "                                                 Cusine  \\\n",
       "S.No                                                      \n",
       "1     [₹ 2,000 for 2 (approx) ,  North Indian, Chinese]   \n",
       "2     [₹ 1,400 for 2 (approx) ,  North Indian, Asian...   \n",
       "3     [₹ 2,000 for 2 (approx) ,  Chinese, North Indian]   \n",
       "4      [₹ 3,000 for 2 (approx) ,  Italian, Continental]   \n",
       "5     [₹ 1,700 for 2 (approx) ,  North Indian, Chinese]   \n",
       "6     [₹ 2,400 for 2 (approx) ,  North Indian, Italian]   \n",
       "7              [₹ 1,800 for 2 (approx) ,  North Indian]   \n",
       "8              [₹ 1,900 for 2 (approx) ,  North Indian]   \n",
       "9     [₹ 2,400 for 2 (approx) ,  North Indian, Italian]   \n",
       "10    [₹ 2,200 for 2 (approx) ,  North Indian, Mughlai]   \n",
       "11               [₹ 800 for 2 (approx) ,  North Indian]   \n",
       "12    [₹ 1,500 for 2 (approx) ,  North Indian, Mughl...   \n",
       "13    [₹ 3,400 for 2 (approx) ,  European, Italian, ...   \n",
       "\n",
       "                                               Location Rating  \\\n",
       "S.No                                                             \n",
       "1                        Connaught Place, Central Delhi    3.5   \n",
       "2                3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "3                Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "4     The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "5                    Gardens Galleria,Sector 38A, Noida      4   \n",
       "6                  Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "7        Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
       "8     Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "9      Vibe by The Lalit Traveller,Sector 35, Faridabad    4.2   \n",
       "10     Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "11                                 Sector 29, Faridabad    3.6   \n",
       "12                                       NIT, Faridabad    4.2   \n",
       "13    DoubleTree By Hilton Gurugram Baani Square,Sec...      4   \n",
       "\n",
       "                                                  image  \n",
       "S.No                                                     \n",
       "1     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9     https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10    https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11    https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12    https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13    https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "  \n",
    "def DINE_OUT(x):\n",
    "\n",
    "#   requesting the url\n",
    "    page= requests.get(x)\n",
    "\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "# empty list to store data\n",
    "    cusine=[]\n",
    "    name=[]\n",
    "    location=[]\n",
    "    rating=[]\n",
    "    z=[]\n",
    "    rank=[]\n",
    "    \n",
    "#      fetching the required data\n",
    "\n",
    "    for i in soup.find_all('img',class_=\"no-img\"):\n",
    "        z.append(i['data-src'])\n",
    "        \n",
    "    for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "        location.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "        name.append(i.text.split()) \n",
    "    \n",
    "    for l in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "        cusine.append(l.text.split(\"|\"))\n",
    "    \n",
    "\n",
    "    for l in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "        rating.append(l.text)\n",
    "            \n",
    "    for k in range(1,len(name)+1):\n",
    "        rank.append(k)\n",
    "            \n",
    "    \n",
    "#     creating the dataframe\n",
    "    DATA=pd.DataFrame({\"S.No\":rank,\"Retaurant\":name,\"Cusine\":cusine,\"Location\":location,\"Rating\":rating,\"image\":z})\n",
    "    DATA.set_index('S.No',inplace =True)\n",
    "\n",
    "    # display the data\n",
    "    return DATA\n",
    "DINE_OUT('https://www.dineout.co.in/delhi-restaurants/buffet-special')  # The final DataFrame shows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935eb07",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "\n",
    "i) Rank\n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fcecc82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUBLICATION</th>\n",
       "      <th>H5-INDEX</th>\n",
       "      <th>H5-MEDIAN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANK</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.</th>\n",
       "      <td>Nature</td>\n",
       "      <td>414</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.</th>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>410</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.</th>\n",
       "      <td>Science</td>\n",
       "      <td>391</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.</th>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>356</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.</th>\n",
       "      <td>The Lancet</td>\n",
       "      <td>345</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96.</th>\n",
       "      <td>Frontiers in Immunology</td>\n",
       "      <td>134</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.</th>\n",
       "      <td>Small</td>\n",
       "      <td>134</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98.</th>\n",
       "      <td>Nature Immunology</td>\n",
       "      <td>133</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.</th>\n",
       "      <td>JAMA Oncology</td>\n",
       "      <td>133</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.</th>\n",
       "      <td>The Lancet Neurology</td>\n",
       "      <td>133</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            PUBLICATION H5-INDEX H5-MEDIAN\n",
       "RANK                                                                      \n",
       "1.                                               Nature      414       607\n",
       "2.                  The New England Journal of Medicine      410       704\n",
       "3.                                              Science      391       564\n",
       "4.    IEEE/CVF Conference on Computer Vision and Pat...      356       583\n",
       "5.                                           The Lancet      345       600\n",
       "...                                                 ...      ...       ...\n",
       "96.                             Frontiers in Immunology      134       177\n",
       "97.                                               Small      134       173\n",
       "98.                                   Nature Immunology      133       210\n",
       "99.                                       JAMA Oncology      133       202\n",
       "100.                               The Lancet Neurology      133       200\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Here improted necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "  \n",
    "\n",
    "def PUBLICATION(x):\n",
    "\n",
    "#   requesting the url\n",
    "    page= requests.get(x)\n",
    "\n",
    "    soup=BeautifulSoup(page.content)\n",
    "    \n",
    "# empty list to store data\n",
    "    rank=[]\n",
    "    publication=[]\n",
    "    h5_index=[]\n",
    "    h5_median=[]\n",
    "    obj=[]\n",
    "\n",
    "#     fetching the required data\n",
    "\n",
    "    for i in soup.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "        publication.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "        rank.append(i.text)\n",
    "        \n",
    "    for i in soup.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_index.append(i.text)\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_median.append(i.text)\n",
    "    \n",
    "    import pandas as pd\n",
    "#     creating a dataframe\n",
    "    DATA=pd.DataFrame({\"RANK\":rank,\"PUBLICATION\":publication,\"H5-INDEX\":h5_index,\"H5-MEDIAN\":h5_median})\n",
    "    \n",
    "    DATA.set_index('RANK',inplace =True)\n",
    "    \n",
    "\n",
    "    \n",
    "#     display the data\n",
    "    return DATA\n",
    "\n",
    "PUBLICATION('https://scholar.google.com/citations?view_op=top_venues&hl=en')# The final DataFrame shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06462d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daebf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
